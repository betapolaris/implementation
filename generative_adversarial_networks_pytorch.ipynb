{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla GAN Applied on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set directory for tensorboard logs\n",
    "log_dir = './logs/vanilla_gan_01'\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation to be applied to the data\n",
    "compose = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Load the MNIST data\n",
    "DATA_PATH = './data/MNIST'\n",
    "data = datasets.MNIST(DATA_PATH, train=True, transform=compose, download=True)\n",
    "\n",
    "# Create data loader\n",
    "batch_size = 100\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to display an image\n",
    "def imshow(image):\n",
    "    image = image * 0.3081 + 0.1307  # Un-normalize\n",
    "    np_image = image.numpy()\n",
    "    plt.imshow(np_image, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "# Utility function to display random images from the dataset\n",
    "def sampleshow(n_samples=10, samples_per_row=10):\n",
    "    loader = torch.utils.data.DataLoader(data, batch_size=n_samples, shuffle=True, num_workers=1)\n",
    "    images, labels = iter(loader).next()\n",
    "    images_grid = utils.make_grid(images, nrow=samples_per_row).permute(1, 2, 0)\n",
    "    imshow(images_grid)\n",
    "    print(labels)\n",
    "    \n",
    "sampleshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator's network\n",
    "# Here, we will use a vanilla neural net\n",
    "class DiscriminatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        n_features = 28 * 28  # size of MNIST data\n",
    "        n_out = 1  # a single number, the probability of the data being real\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 1024), \n",
    "            nn.LeakyReLU(0.2), \n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512), \n",
    "            nn.LeakyReLU(0.2), \n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 256), \n",
    "            nn.LeakyReLU(0.2), \n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(256, n_out), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator's network\n",
    "# We will also use a vanilla neural network for this one\n",
    "class GeneratorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        n_features = 100\n",
    "        n_out = 28 * 28\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256), \n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(256, 512), \n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024), \n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, n_out), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create networks\n",
    "discriminator = DiscriminatorNet().to(device)\n",
    "generator = GeneratorNet().to(device)\n",
    "\n",
    "# Create optimizers\n",
    "learning_rate = 0.0002\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "loss = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various utility functions\n",
    "def images_to_vectors(images):\n",
    "    return images.view(images.size(0), 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 1, 28, 28)\n",
    "\n",
    "def generate_noise(size):\n",
    "    return torch.randn(size, 100, device=device)  # set requires_grad = True?\n",
    "\n",
    "def real_target(size):\n",
    "    return torch.ones(size, 1, device=device)\n",
    "\n",
    "def fake_target(size):\n",
    "    return torch.zeros(size, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the discriminator once\n",
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Train on real data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    \n",
    "    # Calculate error and propagate backward\n",
    "    error_real = loss(prediction_real, real_target(real_data.size(0)))\n",
    "    error_real.backward(retain_graph=True)\n",
    "    \n",
    "    # Train on fake data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    \n",
    "    # Calculate error and propagate backward\n",
    "    error_fake = loss(prediction_fake, fake_target(fake_data.size(0)))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # Take optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = ((prediction_real >= 0.5).sum() + (prediction_fake < 0.5).sum()).double() / (len(prediction_real) + len(prediction_fake))\n",
    "    \n",
    "    # Return error and predictions\n",
    "    return error_real + error_fake, prediction_real, prediction_fake, accuracy\n",
    "\n",
    "# Function to train the generator once\n",
    "def train_generator(optimizer, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get discriminator's prediction\n",
    "    prediction = discriminator(fake_data)\n",
    "    \n",
    "    # Calculate error and propagate backward\n",
    "    error = loss(prediction, real_target(prediction.size(0)))\n",
    "    error.backward()\n",
    "    \n",
    "    # Take optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 200\n",
    "\n",
    "# Autosave settings\n",
    "d_save_path = 'models/vgan_discriminator_01.pth'\n",
    "g_save_path = 'models/vgan_generator_01.pth'\n",
    "load_parameters_before_training = True\n",
    "\n",
    "if load_parameters_before_training:\n",
    "    if os.path.exists(d_save_path):\n",
    "        discriminator.load_state_dict(torch.load(d_save_path))\n",
    "        print(\"Discriminator loaded successfully\")\n",
    "    if os.path.exists(g_save_path):\n",
    "        generator.load_state_dict(torch.load(g_save_path))\n",
    "        print(\"Generator loaded successfully\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_id, (real_batch,_) in enumerate(data_loader):\n",
    "        # Generate real data\n",
    "        real_data = images_to_vectors(real_batch).to(device)\n",
    "        \n",
    "        # Generate fake data\n",
    "        noise = generate_noise(real_batch.size(0))\n",
    "        fake_data = generator(noise).detach()\n",
    "        \n",
    "        # Train discriminator first\n",
    "        d_error, prediction_real, prediction_fake, acc = train_discriminator(d_optimizer, real_data, fake_data)\n",
    "        \n",
    "        # Generate a new batch of fake data\n",
    "        noise = generate_noise(real_batch.size(0))\n",
    "        fake_data = generator(noise)\n",
    "        \n",
    "        # Train generator\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "        \n",
    "    # Log data\n",
    "    print(\"Epoch\", epoch, \"\\tAccuracy\", acc.item())\n",
    "    writer.add_scalar(\"discriminator_accuracy\", acc, epoch)\n",
    "    writer.add_scalar(\"discriminator_error\", d_error, epoch)\n",
    "    writer.add_scalar(\"generator_error\", g_error, epoch)\n",
    "        \n",
    "    # Occasionally see how the generator is doing\n",
    "    if epoch % 10 == 9:\n",
    "        with torch.no_grad():\n",
    "            noise = generate_noise(10)\n",
    "            fake_data = generator(noise)\n",
    "            fake_images = vectors_to_images(fake_data).cpu()\n",
    "            grid = utils.make_grid(fake_images, nrow=10).permute(1, 2, 0)\n",
    "            imshow(grid)\n",
    "            \n",
    "        # Save models' parameters\n",
    "        torch.save(discriminator.state_dict(), d_save_path)\n",
    "        torch.save(generator.state_dict(), g_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
