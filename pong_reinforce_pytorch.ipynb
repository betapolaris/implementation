{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Pong Using Only Pixel Values ft. Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time, os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create directory for tensorlog\n",
    "# Make sure to use a new directory for every new run\n",
    "log_dir = 'logs/pong_pg/pong_pg_01'\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Use GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env.seed(seed)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to grab the current game screen and return it as a 2D numpy array\n",
    "def get_image(env):\n",
    "    image = env.render(mode='rgb_array')\n",
    "    image = image.astype(np.float32) / 255.0  # convert to float and scale to the range [0,1]\n",
    "    return image.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the policy network. Here, we will use a convolutional neural network\n",
    "# that will take an entire screen of game state and suggest an action from that.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_channels=3, input_height=210, input_width=160, output_size=6):\n",
    "        \"\"\"Initialize the network\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Can try using pooling layers here\n",
    "        flat_size = 32 * (self.conv2d_size_out(self.conv2d_size_out(input_height, 8, 4) // 2, 4, 2) // 2) * \\\n",
    "                    (self.conv2d_size_out(self.conv2d_size_out(input_width, 8, 4) // 2, 4, 2) // 2)\n",
    "        self.fc1 = nn.Linear(flat_size, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "        \n",
    "    def conv2d_size_out(self, size, kernel_size, stride):\n",
    "        \"\"\"Utility function to calculate size of dimension after convolution\"\"\"\n",
    "        return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Make a forward pass\"\"\"\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the agent\n",
    "class Agent:\n",
    "    def __init__(self, learning_rate=0.001, n_actions=6):\n",
    "        \"\"\"Initialize agent\"\"\"\n",
    "        self.learning_Rate = learning_rate\n",
    "        self.n_actions = n_actions\n",
    "        self.policy = PolicyNetwork().to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state = state.to(device)\n",
    "        probs = self.policy(state)\n",
    "        action = np.random.choice(self.n_actions, p=probs.to('cpu').detach().squeeze(0).numpy())\n",
    "        log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        return action, log_prob\n",
    "    \n",
    "    def get_probs(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.policy(state)\n",
    "        return probs\n",
    "    \n",
    "    def update(self, rewards, log_probs):\n",
    "        # Convert to tensors\n",
    "        rewards_tensor = torch.from_numpy(rewards).to(device)\n",
    "        log_probs_tensor = torch.stack(log_probs).to(device)\n",
    "        \n",
    "        # Reset parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Compute \"loss\" function\n",
    "        loss = torch.mul(rewards_tensor, -log_probs_tensor).sum()\n",
    "        \n",
    "        # Perform backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_parameters(self, path):\n",
    "        \"\"\"Save policy's parameters\"\"\"\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "        \n",
    "    def load_parameters(self, path):\n",
    "        \"\"\"Load policy's parameters\"\"\"\n",
    "        self.policy.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to discount a sequence of rewards, assume input is a 1D numpy array\n",
    "def discount_rewards(rewards, discount_rate=0.9):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_reward = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        discounted_rewards[i] = rewards[i] + discount_rate * total_reward\n",
    "        total_reward = discounted_rewards[i]\n",
    "    return discounted_rewards\n",
    "\n",
    "# A function to normalize rewards, assume input is a 1D numpy array\n",
    "def normalize_rewards(rewards):\n",
    "    return (rewards - rewards.mean()) / rewards.std()\n",
    "\n",
    "# A function to simulate an episode\n",
    "def simulate(env, agent, render=False, fps=30, max_steps=10000, detailed=False):\n",
    "    seconds_per_frame = 1 / fps\n",
    "    total_reward = 0\n",
    "    env.reset()\n",
    "    image = get_image(env)\n",
    "    prev_image = image\n",
    "    state = image - prev_image\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(seconds_per_frame)\n",
    "            \n",
    "        if detailed:\n",
    "            probs = agent.get_probs(state)\n",
    "            print(probs)\n",
    "            \n",
    "        action, log_prob = agent.get_action(state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "        else:\n",
    "            prev_image = image\n",
    "            image = get_image(env)\n",
    "            state = image - prev_image\n",
    "    \n",
    "    if detailed:\n",
    "        print(\"Simulation complete - total reward:\", total_reward)\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent hyperparameters\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(learning_rate)\n",
    "\n",
    "# Autosave settings\n",
    "save_parameters = True\n",
    "save_interval = 10\n",
    "save_path = 'models/pong_pg_v1.pth'\n",
    "load_parameters_before_training = True\n",
    "\n",
    "if load_parameters_before_training and os.path.exists(save_path):\n",
    "    agent.load_parameters(save_path)\n",
    "    print(\"Parameters loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "n_epochs = 10000\n",
    "discount_rate = 0.99\n",
    "\n",
    "# Option to show the agent in training\n",
    "show_simulation = False\n",
    "epoch_per_simulation = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    score = 0\n",
    "    episode_length = 0\n",
    "    done = False\n",
    "    \n",
    "    # Initialize environment\n",
    "    env.reset()\n",
    "    image = get_image(env)\n",
    "    prev_image = image\n",
    "    state = image - prev_image\n",
    "    \n",
    "    # Collect trajectory of a full episode\n",
    "    while not done:\n",
    "        # Determine action\n",
    "        action, log_prob = agent.get_action(state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Collect reward obtained and the log probability of the selected action\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        # Update state\n",
    "        prev_image = image\n",
    "        image = get_image(env)\n",
    "        state = image - prev_image\n",
    "        \n",
    "        # Keep track of current score\n",
    "        score += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "    # Discount and normalize rewards\n",
    "    rewards = discount_rewards(np.array(rewards), discount_rate=discount_rate)\n",
    "    rewards = normalize_rewards(rewards)\n",
    "    \n",
    "    # Update the policy\n",
    "    agent.update(rewards, log_probs)\n",
    "    \n",
    "    # Track performance in TensorBoard\n",
    "    writer.add_scalar('score', score, epoch)\n",
    "    writer.add_scalar('episode_length', episode_length, epoch)\n",
    "    print(\"Epoch:\", epoch, \"\\tScore:\", score, \"\\tEpisode Length:\", episode_length)\n",
    "    \n",
    "    # Save model parameters\n",
    "    if save_parameters:\n",
    "        if epoch % save_interval == 0:\n",
    "            agent.save_parameters(save_path)\n",
    "    \n",
    "    # Simulate agent (optional)\n",
    "    if show_simulation and epoch % epoch_per_simulation == 0:\n",
    "        simulate(env, agent, render=True, fps=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate(env, agent, render=True, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
