{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Pong Using Only Pixel Values ft. Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time, os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Create directory for tensorlog\n",
    "# Make sure to use a new directory for every new run\n",
    "log_dir = 'logs/pong_pg_test'\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env.seed(seed)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to grab the current game screen and return it as a 2D numpy array\n",
    "def get_image(env):\n",
    "    image = env.render(mode='rgb_array')\n",
    "    image = image.astype(np.float32) / 255.0  # convert to float and scale to the range [0,1]\n",
    "    image = np.dot(image, [0.299, 0.587, 0.114])  # convert to grayscale\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the policy network. Here, we will use a convolutional neural network\n",
    "# that will take an entire screen of game state and suggest an action from that.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_height=210, input_width=160, output_size=6):\n",
    "        \"\"\"Initialize the network\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        # Can try using pooling layers here\n",
    "        flat_size = 32 * (self.conv2d_size_out(self.conv2d_size_out(input_height, 8, 4) // 2, 4, 2) // 2) * \\\n",
    "                    (self.conv2d_size_out(self.conv2d_size_out(input_width, 8, 4) // 2, 4, 2) // 2)\n",
    "        self.fc1 = nn.Linear(flat_size, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "        \n",
    "    def conv2d_size_out(self, size, kernel_size, stride):\n",
    "        \"\"\"Utility function to calculate size of dimension after convolution\"\"\"\n",
    "        return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Make a forward pass\"\"\"\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the agent\n",
    "class Agent:\n",
    "    def __init__(self, learning_rate=0.001, n_actions=6):\n",
    "        \"\"\"Initialize agent\"\"\"\n",
    "        self.learning_Rate = learning_rate\n",
    "        self.n_actions = n_actions\n",
    "        self.policy = PolicyNetwork().to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).unsqueeze(0)\n",
    "        state = state.to(device)\n",
    "        probs = self.policy(state)\n",
    "        action = np.random.choice(self.n_actions, p=probs.to('cpu').detach().squeeze(0).numpy())\n",
    "        log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        state = state.to('cpu')\n",
    "        return action, log_prob\n",
    "    \n",
    "    def get_probs(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).unsqueeze(0)\n",
    "        state = state.to(device)\n",
    "        probs = self.policy(state)\n",
    "        state = state.to('cpu')\n",
    "        return probs\n",
    "    \n",
    "    def update(self, rewards, log_probs, end_flags, discount_rate=0.9):\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = [0] * len(rewards)\n",
    "        total_reward = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if end_flags[i] is True:\n",
    "                total_reward = 0\n",
    "            discounted_rewards[i] = rewards[i] + discount_rate * total_reward\n",
    "            total_reward = discounted_rewards[i]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        discounted_rewards = torch.tensor(discounted_rewards).to(device)\n",
    "        log_probs = torch.stack(log_probs).to(device)\n",
    "        \n",
    "        # Normalize rewards, this should speed up training\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        # Reset parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Compute \"loss\" function\n",
    "        loss = torch.mul(discounted_rewards, -log_probs).mean()\n",
    "        \n",
    "        # Perform backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def save_parameters(self, path):\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "        \n",
    "    def load_parameters(self, path):\n",
    "        self.policy.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to simulate an episode\n",
    "def simulate(env, agent, render=False, fps=30, max_steps=10000, detailed=False):\n",
    "    seconds_per_frame = 1 / fps\n",
    "    total_reward = 0\n",
    "    env.reset()\n",
    "    image = get_image(env)\n",
    "    prev_image = image\n",
    "    state = image - prev_image\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(seconds_per_frame)\n",
    "            \n",
    "        if detailed:\n",
    "            probs = agent.get_probs(state)\n",
    "            print(probs)\n",
    "            \n",
    "        action, log_prob = agent.get_action(state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "        else:\n",
    "            prev_image = image\n",
    "            image = get_image(env)\n",
    "            state = image - prev_image\n",
    "    \n",
    "    if detailed:\n",
    "        print(\"Simulation complete - total reward:\", total_reward)\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent hyperparameters\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Initialize the agent\n",
    "agent = Agent(learning_rate)\n",
    "\n",
    "# Autosave settings\n",
    "save_parameters = True\n",
    "save_interval = 10\n",
    "save_path = 'models/pg_pong_test.pth'\n",
    "load_parameters_before_training = True\n",
    "\n",
    "if load_parameters_before_training and os.path.exists(save_path):\n",
    "    agent.load_parameters(save_path)\n",
    "    print(\"Parameters loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "n_epochs = 10000\n",
    "max_steps = 5000\n",
    "discount_rate = 0.99\n",
    "\n",
    "# Option to show the agent in training\n",
    "show_simulation = False\n",
    "epoch_per_simulation = 10\n",
    "\n",
    "# Initialize environment\n",
    "env.reset()\n",
    "image = get_image(env)\n",
    "prev_image = image\n",
    "state = image - prev_image\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    end_flags = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action, log_prob = agent.get_action(state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        end_flags.append(done)\n",
    "\n",
    "        if done:\n",
    "            # This episode is finished, start a new one\n",
    "            env.reset()\n",
    "            image = get_image(env)\n",
    "            prev_image = image\n",
    "            state = image - prev_image\n",
    "        else:\n",
    "            prev_image = image\n",
    "            image = get_image(env)\n",
    "            state = image - prev_image\n",
    "        \n",
    "    # Update the policy\n",
    "    agent.update(rewards, log_probs, end_flags, discount_rate)\n",
    "    \n",
    "    # Track performance in TensorBoard\n",
    "    simulated_reward = simulate(env, agent)\n",
    "    writer.add_scalar('simulation_reward', simulated_reward, epoch)\n",
    "    print(\"Epoch:\", epoch, \"\\tReward:\", simulated_reward)\n",
    "    \n",
    "    # Save model parameters\n",
    "    if save_parameters:\n",
    "        if epoch % save_interval == 0:\n",
    "            agent.save_parameters(save_path)\n",
    "    \n",
    "    # Simulate agent (optional)\n",
    "    if show_simulation and epoch % epoch_per_simulation == 0:\n",
    "        simulate(env, agent, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate(env, agent, render=True, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
