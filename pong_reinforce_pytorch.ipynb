{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Pong Using Only Pixel Values ft. Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env.seed(seed)\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to grab the current game screen and return it as a 2D numpy array\n",
    "def get_state(env):\n",
    "    image = env.render(mode='rgb_array')\n",
    "    image = image.astype(np.float32) / 255.0  # convert to float and scale to the range [0,1]\n",
    "    image = np.dot(image, [0.299, 0.587, 0.114])  # convert to grayscale\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the policy network. Here, we will use a convolutional neural network\n",
    "# that will take an entire screen of game state and suggest an action from that.\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_channels=1, input_height=210, input_width=160, output_size=6):\n",
    "        \"\"\"Initialize the network\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "        # Can try using pooling layers here\n",
    "        flat_size = 32 * self.conv2d_size_out(self.conv2d_size_out(input_height, 8, 4), 4, 2) * \\\n",
    "                    self.conv2d_size_out(self.conv2d_size_out(input_width, 8, 4), 4, 2)\n",
    "        self.fc1 = nn.Linear(flat_size, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "        \n",
    "    def conv2d_size_out(self, size, kernel_size, stride):\n",
    "        \"\"\"Utility function to calculate size of dimension after convolution\"\"\"\n",
    "        return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Make a forward pass\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for the agent\n",
    "class Agent:\n",
    "    def __init__(self, learning_rate=0.01, n_actions=6):\n",
    "        \"\"\"Initialize agent\"\"\"\n",
    "        self.learning_Rate = learning_rate\n",
    "        self.n_actions = n_actions\n",
    "        self.policy = PolicyNetwork().to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).unsqueeze(0)\n",
    "        state = state.to(device)\n",
    "        probs = self.policy(state)\n",
    "        action = np.random.choice(self.n_actions, p=probs.to('cpu').detach().squeeze(0).numpy())\n",
    "        log_prob = torch.log(probs.squeeze(0)[action])\n",
    "        return action, log_prob\n",
    "    \n",
    "    def update(self, all_rewards, all_log_probs, discount_rate=0.9):\n",
    "        # Compute discounted rewards\n",
    "        all_discounted_rewards = []\n",
    "        for rewards in all_rewards:\n",
    "            total_reward = 0\n",
    "            discounted_rewards = [0] * len(rewards)\n",
    "            for i in reversed(range(len(rewards))):\n",
    "                discounted_rewards[i] = rewards[i] + discount_rate * total_reward\n",
    "                total_reward = discounted_rewards[i]\n",
    "            all_discounted_rewards.append(discounted_rewards)\n",
    "            \n",
    "        # Stack all rewards and log probs\n",
    "        flat_discounted_rewards = [r for rewards in all_discounted_rewards for r in rewards]\n",
    "        flat_log_probs = [lp for log_probs in all_log_probs for lp in log_probs]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        discounted_rewards = torch.tensor(flat_discounted_rewards).to(device)\n",
    "        log_probs = torch.stack(flat_log_probs).to(device)\n",
    "        \n",
    "        # Normalize rewards, this should speed up training\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "        \n",
    "        # Reset parameter gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Compute \"loss\" function\n",
    "        loss = torch.mul(discounted_rewards, -log_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to simulate an episode\n",
    "def simulate(env, agent, render=False, fps=30, max_steps=10000):\n",
    "    seconds_per_frame = 1 / fps\n",
    "    total_reward = 0\n",
    "    env.reset()\n",
    "    state = get_state(env)\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(seconds_per_frame)\n",
    "            \n",
    "        action, log_prob = agent.get_action(state)\n",
    "        _, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            env.close()\n",
    "            break\n",
    "        else:\n",
    "            state = get_state(env)\n",
    "            \n",
    "    print(\"Simulation complete - total reward:\", total_reward)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: -21.0\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "n_epochs = 1\n",
    "episodes_per_epoch = 1\n",
    "max_steps = 10000\n",
    "discount_rate = 0.9\n",
    "\n",
    "# For tracking average rewards\n",
    "avg_rewards = []\n",
    "\n",
    "# Option to show the agent in training\n",
    "show_simulation = False\n",
    "epoch_per_simulation = 25\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    all_rewards = []\n",
    "    all_log_probs = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for episode in range(episodes_per_epoch):\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        \n",
    "        env.reset()\n",
    "        state = get_state(env)\n",
    "        for step in range(max_steps):\n",
    "            action, log_prob = agent.get_action(state)\n",
    "            _, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                state = get_state(env)\n",
    "                \n",
    "        all_rewards.append(rewards)\n",
    "        all_log_probs.append(log_probs)\n",
    "        \n",
    "    # Update the policy\n",
    "    agent.update(all_rewards, all_log_probs, discount_rate)\n",
    "    \n",
    "    # Track average rewards\n",
    "    avg_rewards.append(total_reward / episodes_per_epoch)\n",
    "    print(\"Epoch {}: {}\".format(epoch, total_reward / episodes_per_epoch))\n",
    "    \n",
    "    # Simulate agent (optional)\n",
    "    if show_simulation and epoch % epoch_per_simulation == 0:\n",
    "        simulate(env, agent, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation complete - total reward: -21.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-21.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(env, agent, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
