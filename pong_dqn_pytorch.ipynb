{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Pong Using Deep Q-Network\n",
    "\n",
    "Author's note: This code does not work. Either there's an implementation error, or it just takes an absurdly long time for the agent to learn the game. I will be switching to policy gradients method and see if that works better. I'm posting this code on GitHub anyway in case I want to work on it sometime in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create environment, use Pong as default environment for now\n",
    "env = gym.make('Pong-v0')\n",
    "env.seed(42)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Create directory for tensorlog\n",
    "# Make sure to use a new directory for every new run\n",
    "log_dir = 'logs/pong_dqn_1'\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the Q-network. It uses a convolutional neural network to predict the Q-values for every\n",
    "# action in a given state. Assume input size is (N, 4, 210, 160).\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_channels=4, input_height=210, input_width=160, output_size=n_actions):\n",
    "        \"\"\"Initialize Q-network\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2)\n",
    "        # No pooling layers?\n",
    "        flat_size = 32 * self.conv2d_size_out(self.conv2d_size_out(input_height, 8, 4), 4, 2) * \\\n",
    "                    self.conv2d_size_out(self.conv2d_size_out(input_width, 8, 4), 4, 2)\n",
    "        self.fc1 = nn.Linear(flat_size, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "\n",
    "    def conv2d_size_out(self, size, kernel_size, stride):\n",
    "        \"\"\"Utility function to calculate size of dimension after convolution\"\"\"\n",
    "        return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Make a forward pass\"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for experience replay. This class will store state transitions (state, action, reward, next_state).\n",
    "# Random memory will be sampled when training the agent. This method supposedly reduces variance.\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, memory_cap):\n",
    "        \"\"\"Initialize memory\"\"\"\n",
    "        self.memory_cap = memory_cap\n",
    "        self.size = 0\n",
    "        self.index = 0\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        \n",
    "    def store(self, state, action, reward, next_state):\n",
    "        \"\"\"Store a transition\"\"\"\n",
    "        if self.size < self.memory_cap:\n",
    "            self.states.append(state)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.next_states.append(next_state)\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.states[self.index] = state\n",
    "            self.actions[self.index] = action\n",
    "            self.rewards[self.index] = reward\n",
    "            self.next_states[self.index] = next_state\n",
    "            self.index = (self.index + 1) % self.memory_cap\n",
    "    \n",
    "    def stack_states(self, i, stack_size):\n",
    "        \"\"\"Utility function to stack the last four states ending at the i-th index\"\"\"\n",
    "        # Check validity of all states that are going to be stacked\n",
    "        for k in range(i - stack_size + 1, i):\n",
    "            if k < 0 or k >= self.size:\n",
    "                # Out of bound\n",
    "                return None\n",
    "            elif self.next_states[k] is None:\n",
    "                # Some of the states come from different episodes\n",
    "                return None\n",
    "            \n",
    "        return torch.cat(self.states[i-stack_size+1:i+1]).unsqueeze(0)\n",
    "    \n",
    "    def sample(self, batch_size, stack_size=4):\n",
    "        \"\"\"Sample memory\"\"\"\n",
    "        states_batch = []\n",
    "        actions_batch = []\n",
    "        rewards_batch = []\n",
    "        next_states_batch = []\n",
    "        random_indices = np.random.permutation(self.size)\n",
    "        for random_index in random_indices:\n",
    "            stacked_states = self.stack_states(random_index, stack_size)\n",
    "            if stacked_states is not None:\n",
    "                states_batch.append(stacked_states)\n",
    "                actions_batch.append(self.actions[random_index])\n",
    "                rewards_batch.append(self.rewards[random_index])\n",
    "                if self.next_states[random_index] is None:\n",
    "                    # This state is a terminal state\n",
    "                    next_states_batch.append(None)\n",
    "                else:\n",
    "                    next_states_batch.append(torch.cat(self.next_states[random_index-stack_size+1:random_index+1]).unsqueeze(0))\n",
    "                if len(states_batch) == batch_size:\n",
    "                    break\n",
    "        if len(states_batch) < batch_size:\n",
    "            return None  # not enough samples\n",
    "        return (states_batch, actions_batch, rewards_batch, next_states_batch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the agent.\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name='Agent', frame_stack_size=4, memory_cap=10000, learning_rate=0.01,  \n",
    "                 epsilon_max=1.0, epsilon_min=0.05, epsilon_decay=10000, gamma=0.99):\n",
    "        \"\"\"Initialize agent\"\"\"\n",
    "        self.name = name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.Q = QNetwork(input_channels=frame_stack_size).to(device)\n",
    "        self.targetQ = QNetwork(input_channels=frame_stack_size).to(device)\n",
    "        self.targetQ.load_state_dict(self.Q.state_dict())\n",
    "        self.memory = ExperienceReplay(memory_cap)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.RMSprop(self.Q.parameters(), learning_rate)\n",
    "        \n",
    "    def get_action(self, state, global_step):\n",
    "        \"\"\"Use epsilon-greedy to determine action\"\"\"\n",
    "        prior_device = state.device\n",
    "        state = state.to(device)\n",
    "        epsilon = max(self.epsilon_min,\n",
    "                      self.epsilon_max - (self.epsilon_max - self.epsilon_min) * global_step / self.epsilon_decay)\n",
    "        if np.random.rand() < epsilon:\n",
    "            # Take a random action\n",
    "            action = torch.tensor([np.random.randint(n_actions)])\n",
    "        else:\n",
    "            # Take the action with the highest Q-value\n",
    "            with torch.no_grad():\n",
    "                Q_values = self.Q(state)\n",
    "                _, action = Q_values.max(dim=1)\n",
    "        action = action.to(prior_device)\n",
    "        state = state.to(prior_device)\n",
    "        return action\n",
    "    \n",
    "    def get_Q_values(self, state):\n",
    "        \"\"\"Get Q-values of a given state\"\"\"\n",
    "        prior_device = state.device\n",
    "        state = state.to(device)\n",
    "        with torch.no_grad():\n",
    "            Q_values = self.Q(state)\n",
    "        state = state.to(prior_device)\n",
    "        return Q_values\n",
    "    \n",
    "    def store(self, state, action, reward, next_state):\n",
    "        '''Store the transition in memory'''\n",
    "        self.memory.store(state, action, reward, next_state)\n",
    "    \n",
    "    def update_parameters(self, batch_size=32, stack_size=4):\n",
    "        '''Sample memory and update parameters'''\n",
    "        # Get samples and convert them to tensors\n",
    "        samples = self.memory.sample(batch_size, stack_size)\n",
    "        if samples is None:\n",
    "            return None  # not enough samples\n",
    "        else:\n",
    "            states_batch, actions_batch, rewards_batch, next_states_batch = samples\n",
    "        states = torch.cat(states_batch)\n",
    "        actions = torch.cat(actions_batch)\n",
    "        rewards = torch.cat(rewards_batch)\n",
    "        non_final_states_mask = [True if next_state is not None else False for next_state in next_states_batch]\n",
    "        non_final_next_states = torch.cat([next_state for next_state in next_states_batch if next_state is not None])\n",
    "        \n",
    "        # Send sample tensors to the specified device\n",
    "        prior_device = states.device\n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        non_final_next_states = non_final_next_states.to(device)\n",
    "        \n",
    "        # Compute predicted Q-values\n",
    "        Q_values = self.Q(states).gather(dim=1, index=actions.unsqueeze(1))\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        targets = rewards.unsqueeze(1).clone().detach()\n",
    "        targets[non_final_states_mask] += self.gamma * self.targetQ(non_final_next_states).max(dim=1, keepdim=True)[0]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss(Q_values, targets)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Send sample tensors back to original device\n",
    "        states =states.to(prior_device)\n",
    "        actions = actions.to(prior_device)\n",
    "        rewards = rewards.to(prior_device)\n",
    "        non_final_next_states = non_final_next_states.to(prior_device)\n",
    "        \n",
    "        # Return loss\n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network to match current Q-network\"\"\"\n",
    "        self.targetQ.load_state_dict(self.Q.state_dict())\n",
    "        \n",
    "    def save_parameters(self, path):\n",
    "        \"\"\"Save model parameters in a file\"\"\"\n",
    "        torch.save(self.Q.state_dict(), path)\n",
    "    \n",
    "    def load_parameters(self, path):\n",
    "        \"\"\"Load model parameters from a file\"\"\"\n",
    "        self.Q.load_state_dict(torch.load(path))\n",
    "        self.targetQ.load_state_dict(self.Q.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input images\n",
    "def preprocess_image(image):\n",
    "    image_gray = np.dot(image, [0.299, 0.587, 0.114])  # what if we just take the mean?\n",
    "    return image_gray.astype(np.float32) / 255.0\n",
    "\n",
    "# Return the current frame as a tensor\n",
    "def get_image(env):\n",
    "    image = env.render(mode='rgb_array')\n",
    "    image = preprocess_image(image)\n",
    "    return torch.from_numpy(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(env, agent, render=False, fps=30, detailed=False):\n",
    "    env.reset()\n",
    "    state = get_image(env)\n",
    "    states_stack = []\n",
    "    for _ in range(states_stack_size):\n",
    "        states_stack.append(get_image(env))\n",
    "        env.step(0)\n",
    "    done = False\n",
    "    score = 0\n",
    "        \n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(1/fps)\n",
    "        \n",
    "        states_tensor = torch.cat(states_stack).unsqueeze(0)\n",
    "        action = agent.get_action(states_tensor, global_step)\n",
    "        \n",
    "        if detailed:\n",
    "            Qs = agent.get_Q_values(states_tensor)\n",
    "            print(Qs.detach().cpu().numpy(), end='\\r')\n",
    "        \n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        next_state = get_image(env)\n",
    "        next_states_stack = states_stack[1:]\n",
    "        next_states_stack.append(next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        states_stack = next_states_stack\n",
    "        \n",
    "    env.close()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent hyperparameters\n",
    "states_stack_size = 4\n",
    "memory_cap = 50000\n",
    "learning_rate = 0.01\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 1000000\n",
    "gamma = 0.99\n",
    "\n",
    "# Initialize agent\n",
    "agent = Agent('PongPong_v0', states_stack_size, memory_cap, learning_rate, \n",
    "              epsilon_max, epsilon_min, epsilon_decay, gamma)\n",
    "\n",
    "# Training hyperparameters\n",
    "global_step = 0\n",
    "frame_skip_count = 3\n",
    "training_length = 20000001\n",
    "update_interval = 10\n",
    "batch_size = 64\n",
    "target_update_interval = 10\n",
    "load_parameters_before_training = True\n",
    "save_parameters = True\n",
    "save_interval = 10000\n",
    "save_path = 'pongpong_v0.pth'\n",
    "simulate = True\n",
    "simulate_interval = 10000\n",
    "\n",
    "# Load model parameters if enabled\n",
    "if load_parameters_before_training:\n",
    "    agent.load_parameters(save_path)\n",
    "\n",
    "# Initialize environment\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "\n",
    "# Initialize states stack\n",
    "state = get_image(env)\n",
    "states_stack = []\n",
    "for _ in range(states_stack_size):\n",
    "    states_stack.append(get_image(env))\n",
    "    env.step(0)\n",
    "initial_states_tensor = torch.cat(states_stack).unsqueeze(0)\n",
    "    \n",
    "# Track expected values and loss over time\n",
    "values = []\n",
    "losses = []\n",
    "scores = []\n",
    "\n",
    "# Main training loop\n",
    "for i in range(training_length):\n",
    "    # Choose action\n",
    "    states_tensor = torch.cat(states_stack).unsqueeze(0)\n",
    "    action = agent.get_action(states_tensor, global_step)\n",
    "    \n",
    "    # Perform the action for a few frames, this should increase the number of samples\n",
    "    for _ in range(frame_skip_count):\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        \n",
    "        # Store transition\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = get_image(env)\n",
    "        agent.store(state, action, torch.tensor([reward]), next_state)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "            next_states_stack = states_stack[1:]\n",
    "            next_states_stack.append(next_state)\n",
    "            states_stack = next_states_stack\n",
    "    \n",
    "    # Update model parameters\n",
    "    if i % update_interval == 0:\n",
    "        loss = agent.update_parameters(batch_size)\n",
    "        if loss is not None:\n",
    "            values.append(agent.get_Q_values(initial_states_tensor).max().item())\n",
    "            losses.append(loss)\n",
    "            global_step += 1\n",
    "            if global_step % target_update_interval == 0:\n",
    "                agent.update_target_network()\n",
    "                writer.add_scalar('initial_value', values[-1], global_step)\n",
    "                writer.add_scalar('loss', losses[-1], global_step)\n",
    "                # print(i, values[-1], losses[-1])\n",
    "    \n",
    "    # Save model parameters\n",
    "    if save_parameters:\n",
    "        if i % save_interval == 0:\n",
    "            agent.save_parameters(save_path)\n",
    "            \n",
    "    # Show agent every once in a while\n",
    "    if simulate:\n",
    "        if i % simulate_interval == 0:\n",
    "            score = simulate_episode(env, agent)\n",
    "            scores.append(score)\n",
    "            \n",
    "    if done:\n",
    "        env.reset()\n",
    "        state = get_image(env)\n",
    "        states_stack = []\n",
    "        for _ in range(states_stack_size):\n",
    "            states_stack.append(get_image(env))\n",
    "            env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.gca()\n",
    "axes.set_ylim([-25,25])\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_episode(env, agent, render=True, detailed=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
